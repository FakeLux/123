{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import markdown2\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# 增强的配置参数\n",
    "SIM_THRESHOLD = {\n",
    "    \"责任免除\": {\"base\": 0.92, \"adjust\": 0.02},  # 基础阈值+动态调整范围\n",
    "    \"术语解释\": {\"base\": 0.88, \"adjust\": 0.03},\n",
    "    \"赔付规则\": {\"base\": 0.95, \"adjust\": 0.01},  # 数值类规则更严格\n",
    "    \"保障相关时间\": {\"base\": 0.95, \"adjust\": 0.01}\n",
    "}\n",
    "\n",
    "# 增强的正则表达式模式\n",
    "FIELD_PATTERNS = {\n",
    "    \"责任免除\": r\"(?:责任免除|免责说明|免责情形)[:：]?\\s*([\\s\\S]*?)(?=(?:保障责任|赔付规则|\\n##|\\n#|$))\",\n",
    "    \"赔付比例\": r\"(?:赔付比例|给付比例)[:：]?\\s*(\\d+%|按[\\d一二三四五六七八九十]+成)\",\n",
    "    \"等待期\": r\"(?:等待期|观察期)[:：]?\\s*(\\d+)\\s*天\",\n",
    "    \"投保年龄\": r\"(?:投保年龄|承保年龄)[:：]?\\s*([\\d\\-～]+)\\s*岁?\",\n",
    "    \"保险金额\": r\"(?:保险金额|基本保额)[:：]?\\s*([\\d,]+)\\s*元\",\n",
    "    \"产品名称\": r\"(?:产品名称|保险产品)[:：]\\s*([^\\n]+)\",\n",
    "    \"保险期间\": r\"(?:保险期间|保障期限)[:：]\\s*([^\\n]+)\",\n",
    "    \"交费期间\": r\"(?:交费期间|缴费期限)[:：]\\s*([^\\n]+)\",\n",
    "    \"犹豫期\": r\"(?:犹豫期|冷静期)[:：]\\s*(\\d+)\\s*天\",\n",
    "    \"免赔额\": r\"(?:免赔额|自付额)[:：]\\s*([^\\n]+)\",\n",
    "    \"赔付次数\": r\"(?:赔付次数|给付次数)[:：]\\s*([^\\n]+)\"\n",
    "}\n",
    "\n",
    "# 文本预处理正则\n",
    "PREPROCESS_PATTERNS = [\n",
    "    (r'[\\u3000\\s]+', ' '),  # 替换全角空格和连续空格\n",
    "    (r'[【】]', ''),        # 移除中文括号\n",
    "    (r'[（）]', '()'),      # 统一括号格式\n",
    "    (r'[:：]\\s*', ': ')     # 统一冒号格式\n",
    "]\n",
    "\n",
    "# 素材类型映射\n",
    "TYPE_MAPPING = {\n",
    "    \"CLAUSE\": \"条款\",\n",
    "    \"HEAD_IMG\": \"头图\",\n",
    "    \"INSURE_NOTICE\": \"投保须知\",\n",
    "    \"INTRODUCE_IMG\": \"图文说明\",\n",
    "    \"LIABILITY_EXCLUSION\": \"免责说明\"\n",
    "}\n",
    "\n",
    "# 规则类型映射\n",
    "RULE_TYPE_MAPPING = {\n",
    "    \"基础产品销售信息\": \"销售信息\",\n",
    "    \"责任免除\": \"免责条款\",\n",
    "    \"赔付规则\": \"赔付规则\",\n",
    "    \"保障相关时间\": \"保障时间\",\n",
    "    \"投保条款\": \"投保条件\",\n",
    "    \"术语解释\": \"术语定义\"\n",
    "}\n",
    "\n",
    "class EnhancedInsuranceRiskDetector:\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"初始化增强版保险素材风险检测器\"\"\"\n",
    "        cache_dir = os.path.join(os.getcwd(), \"model_cache\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"使用设备: {self.device}\")\n",
    "        \n",
    "        # 加载模型并设置设备\n",
    "        self.model = SentenceTransformer(\n",
    "            model_path, \n",
    "            cache_folder=cache_dir,\n",
    "            device=self.device\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # 注册规则检查处理器\n",
    "        self.rule_handlers = {\n",
    "            \"销售信息\": self.check_sales_info,\n",
    "            \"免责条款\": self.check_liability_exclusion,\n",
    "            \"赔付规则\": self.check_payment_rule,\n",
    "            \"保障时间\": self.check_time_rule,\n",
    "            \"投保条件\": self.check_insurance_terms,\n",
    "            \"术语定义\": self.check_term_definition\n",
    "        }\n",
    "        \n",
    "        # 编译预处理正则\n",
    "        self.preprocess_regex = [(re.compile(pat), repl) for pat, repl in PREPROCESS_PATTERNS]\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"文本预处理：清洗和标准化\"\"\"\n",
    "        for regex, repl in self.preprocess_regex:\n",
    "            text = regex.sub(repl, text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def detect_risk(self, material_data: Dict[str, Any], rule: str) -> bool:\n",
    "        \"\"\"增强版风险检测\"\"\"\n",
    "        rule_type = self.infer_rule_type(rule)\n",
    "        handler = self.rule_handlers.get(rule_type)\n",
    "        \n",
    "        if not handler:\n",
    "            print(f\"警告：未找到匹配的规则处理器，规则类型: {rule_type}，规则内容: {rule}\")\n",
    "            return True\n",
    "        \n",
    "        # 筛选并预处理相关素材\n",
    "        relevant_docs = []\n",
    "        for doc in material_data[\"documents\"]:\n",
    "            if doc[\"type\"] in TYPE_MAPPING.values():\n",
    "                processed_doc = doc.copy()\n",
    "                processed_doc[\"text\"] = self.preprocess_text(doc[\"text\"])\n",
    "                processed_doc[\"fields\"] = {k: self.preprocess_text(v) for k, v in doc[\"fields\"].items()}\n",
    "                relevant_docs.append(processed_doc)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            print(f\"警告：素材包 {material_data['material_id']} 中缺少关键素材类型\")\n",
    "            return False\n",
    "        \n",
    "        return handler(relevant_docs)\n",
    "    \n",
    "    def check_sales_info(self, docs: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"增强版销售信息检查\"\"\"\n",
    "        sales_fields = [\"产品名称\", \"保险期间\", \"交费期间\", \"保险金额\"]\n",
    "        field_values = defaultdict(set)\n",
    "        \n",
    "        for doc in docs:\n",
    "            for field in sales_fields:\n",
    "                value = doc[\"fields\"].get(field, \"\")\n",
    "                if value:\n",
    "                    # 标准化处理\n",
    "                    norm_value = self.normalize_field(field, value)\n",
    "                    if norm_value:\n",
    "                        field_values[field].add(norm_value)\n",
    "        \n",
    "        # 检查一致性\n",
    "        conflicts = []\n",
    "        for field, values in field_values.items():\n",
    "            if len(values) > 1:\n",
    "                conflicts.append((field, values))\n",
    "        \n",
    "        if conflicts:\n",
    "            print(\"销售信息冲突发现:\")\n",
    "            for field, values in conflicts:\n",
    "                print(f\"  {field}: {values}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def check_liability_exclusion(self, docs: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"增强版责任免除检查\"\"\"\n",
    "        exclusion_texts = []\n",
    "        for doc in docs:\n",
    "            if \"责任免除\" in doc[\"fields\"]:\n",
    "                text = doc[\"fields\"][\"责任免除\"]\n",
    "                if text.strip():\n",
    "                    exclusion_texts.append(text)\n",
    "        \n",
    "        if len(exclusion_texts) < 2:\n",
    "            return True\n",
    "            \n",
    "        # 分段处理长文本\n",
    "        segmented_texts = []\n",
    "        for text in exclusion_texts:\n",
    "            if len(text) > 200:  # 长文本分段\n",
    "                segments = self.split_text(text, max_length=200)\n",
    "                segmented_texts.extend(segments)\n",
    "            else:\n",
    "                segmented_texts.append(text)\n",
    "        \n",
    "        # 计算动态阈值\n",
    "        avg_length = sum(len(t) for t in segmented_texts) / len(segmented_texts)\n",
    "        threshold = self.get_dynamic_threshold(\"责任免除\", avg_length)\n",
    "        \n",
    "        # 计算相似度\n",
    "        embeddings = self.model.encode(\n",
    "            segmented_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        cos_scores = util.cos_sim(embeddings, embeddings)\n",
    "        cos_scores = cos_scores.cpu().numpy()\n",
    "        np.fill_diagonal(cos_scores, 1.0)\n",
    "        \n",
    "        min_similarity = np.min(cos_scores)\n",
    "        print(f\"责任免除内容最小相似度: {min_similarity:.4f}，动态阈值: {threshold:.4f}\")\n",
    "        \n",
    "        return min_similarity >= threshold\n",
    "    \n",
    "    def check_payment_rule(self, docs: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"增强版赔付规则检查\"\"\"\n",
    "        payment_fields = [\"赔付比例\", \"免赔额\", \"赔付次数\"]\n",
    "        field_values = defaultdict(set)\n",
    "        \n",
    "        for doc in docs:\n",
    "            for field in payment_fields:\n",
    "                value = doc[\"fields\"].get(field, \"\")\n",
    "                if value:\n",
    "                    norm_value = self.normalize_field(field, value)\n",
    "                    if norm_value:\n",
    "                        field_values[field].add(norm_value)\n",
    "        \n",
    "        # 检查一致性\n",
    "        conflicts = []\n",
    "        for field, values in field_values.items():\n",
    "            if len(values) > 1:\n",
    "                conflicts.append((field, values))\n",
    "        \n",
    "        if conflicts:\n",
    "            print(\"赔付规则冲突发现:\")\n",
    "            for field, values in conflicts:\n",
    "                print(f\"  {field}: {values}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def check_time_rule(self, docs: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"增强版保障时间检查\"\"\"\n",
    "        time_fields = [\"等待期\", \"犹豫期\"]\n",
    "        field_values = defaultdict(set)\n",
    "        \n",
    "        for doc in docs:\n",
    "            for field in time_fields:\n",
    "                value = doc[\"fields\"].get(field, \"\")\n",
    "                if value:\n",
    "                    # 提取数值部分\n",
    "                    match = re.search(r'(\\d+)', value)\n",
    "                    if match:\n",
    "                        field_values[field].add(match.group(1))\n",
    "        \n",
    "        # 检查一致性\n",
    "        conflicts = []\n",
    "        for field, values in field_values.items():\n",
    "            if len(values) > 1:\n",
    "                conflicts.append((field, values))\n",
    "        \n",
    "        if conflicts:\n",
    "            print(\"保障时间冲突发现:\")\n",
    "            for field, values in conflicts:\n",
    "                print(f\"  {field}: {values}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def check_insurance_terms(self, docs: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"增强版投保条件检查\"\"\"\n",
    "        age_values = set()\n",
    "        health_values = set()\n",
    "        \n",
    "        for doc in docs:\n",
    "            # 检查投保年龄\n",
    "            age_str = doc[\"fields\"].get(\"投保年龄\", \"\")\n",
    "            if age_str:\n",
    "                # 提取年龄范围\n",
    "                match = re.search(r'(\\d+)\\s*[-～至]?\\s*(\\d+)?\\s*岁?', age_str)\n",
    "                if match:\n",
    "                    min_age = match.group(1)\n",
    "                    max_age = match.group(2) if match.group(2) else min_age\n",
    "                    age_values.add(f\"{min_age}-{max_age}\")\n",
    "            \n",
    "            # 检查健康告知要求\n",
    "            health_text = doc[\"text\"]\n",
    "            health_matches = re.finditer(r'(健康告知|健康状况)[:：](.+?)(?=\\n##|\\n#|$)', health_text)\n",
    "            for match in health_matches:\n",
    "                if match.group(2).strip():\n",
    "                    health_values.add(match.group(2).strip())\n",
    "        \n",
    "        # 检查一致性\n",
    "        conflicts = []\n",
    "        if len(age_values) > 1:\n",
    "            conflicts.append((\"投保年龄\", age_values))\n",
    "        if len(health_values) > 1:\n",
    "            conflicts.append((\"健康告知\", health_values))\n",
    "        \n",
    "        if conflicts:\n",
    "            print(\"投保条件冲突发现:\")\n",
    "            for field, values in conflicts:\n",
    "                print(f\"  {field}: {values}\")\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def check_term_definition(self, docs: List[Dict[str, Any]]) -> bool:\n",
    "        \"\"\"增强版术语定义检查\"\"\"\n",
    "        term_texts = []\n",
    "        for doc in docs:\n",
    "            text = doc[\"text\"]\n",
    "            # 提取术语定义段落\n",
    "            term_paragraphs = re.finditer(r'(?:^|\\n)([^\\n：:]+?)\\s*[:：]\\s*([^\\n]+)', text)\n",
    "            for match in term_paragraphs:\n",
    "                term, definition = match.groups()\n",
    "                if len(definition) > 10:  # 只考虑较长的定义\n",
    "                    term_texts.append(f\"{term}: {definition}\")\n",
    "        \n",
    "        if len(term_texts) < 2:\n",
    "            return True\n",
    "            \n",
    "        # 计算动态阈值\n",
    "        avg_length = sum(len(t) for t in term_texts) / len(term_texts)\n",
    "        threshold = self.get_dynamic_threshold(\"术语解释\", avg_length)\n",
    "        \n",
    "        # 计算相似度\n",
    "        embeddings = self.model.encode(\n",
    "            term_texts,\n",
    "            convert_to_tensor=True,\n",
    "            device=self.device\n",
    "        )\n",
    "        cos_scores = util.cos_sim(embeddings, embeddings)\n",
    "        cos_scores = cos_scores.cpu().numpy()\n",
    "        np.fill_diagonal(cos_scores, 1.0)\n",
    "        \n",
    "        min_similarity = np.min(cos_scores)\n",
    "        print(f\"术语定义最小相似度: {min_similarity:.4f}，动态阈值: {threshold:.4f}\")\n",
    "        \n",
    "        return min_similarity >= threshold\n",
    "    \n",
    "    def split_text(self, text: str, max_length: int = 200) -> List[str]:\n",
    "        \"\"\"智能分段文本\"\"\"\n",
    "        sentences = re.split(r'(?<=[。！？；])', text)\n",
    "        segments = []\n",
    "        current_segment = \"\"\n",
    "        \n",
    "        for sent in sentences:\n",
    "            if len(current_segment) + len(sent) <= max_length:\n",
    "                current_segment += sent\n",
    "            else:\n",
    "                if current_segment:\n",
    "                    segments.append(current_segment)\n",
    "                current_segment = sent\n",
    "        \n",
    "        if current_segment:\n",
    "            segments.append(current_segment)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def get_dynamic_threshold(self, field: str, avg_length: float) -> float:\n",
    "        \"\"\"获取动态调整的相似度阈值\"\"\"\n",
    "        base = SIM_THRESHOLD[field][\"base\"]\n",
    "        adjust = SIM_THRESHOLD[field][\"adjust\"]\n",
    "        \n",
    "        # 长度越长，允许的阈值调整范围越大\n",
    "        length_factor = min(1.0, avg_length / 300)  # 300字符为基准\n",
    "        dynamic_adjust = adjust * length_factor\n",
    "        \n",
    "        return base + dynamic_adjust\n",
    "    \n",
    "    def normalize_field(self, field: str, value: str) -> str:\n",
    "        \"\"\"标准化字段值\"\"\"\n",
    "        if field in [\"保险金额\", \"免赔额\"]:\n",
    "            # 统一金额格式，如\"10,000元\" -> \"10000元\"\n",
    "            return re.sub(r'[^\\d元]', '', value)\n",
    "        elif field == \"投保年龄\":\n",
    "            # 统一年龄格式，如\"30-50岁\" -> \"30-50\"\n",
    "            return re.sub(r'[^\\d\\-～]', '', value)\n",
    "        elif field == \"赔付比例\":\n",
    "            # 统一比例格式，如\"80%\" -> \"80%\"\n",
    "            return re.sub(r'[^\\d%成]', '', value)\n",
    "        return value\n",
    "    \n",
    "    def infer_rule_type(self, rule: str) -> str:\n",
    "        \"\"\"从规则文本推断规则类型\"\"\"\n",
    "        for keyword, rule_type in RULE_TYPE_MAPPING.items():\n",
    "            if keyword in rule:\n",
    "                return rule_type\n",
    "        return \"其他\"\n",
    "    \n",
    "    def parse_material(self, material_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"增强版素材解析\"\"\"\n",
    "        material_data = {\n",
    "            \"material_id\": os.path.basename(material_path),\n",
    "            \"documents\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"正在解析素材包: {material_data['material_id']}\")\n",
    "        \n",
    "        for sub_dir in os.listdir(material_path):\n",
    "            sub_dir_full = os.path.join(material_path, sub_dir)\n",
    "            if os.path.isdir(sub_dir_full) and sub_dir in TYPE_MAPPING:\n",
    "                doc_type = TYPE_MAPPING[sub_dir]\n",
    "                print(f\"  发现文档类型: {doc_type}\")\n",
    "                \n",
    "                for root, dirs, files in os.walk(sub_dir_full):\n",
    "                    for file in files:\n",
    "                        if file.endswith('.md'):\n",
    "                            md_path = os.path.join(root, file)\n",
    "                            try:\n",
    "                                text = self.parse_markdown(md_path)\n",
    "                                fields = self.extract_fields(text, doc_type)\n",
    "                                material_data[\"documents\"].append({\n",
    "                                    \"type\": doc_type,\n",
    "                                    \"path\": md_path,\n",
    "                                    \"text\": text,\n",
    "                                    \"fields\": fields\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"    解析文件失败: {file}, 错误: {e}\")\n",
    "        \n",
    "        return material_data\n",
    "    \n",
    "    def parse_markdown(self, file_path: str) -> str:\n",
    "        \"\"\"将 Markdown 文件转为纯文本\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            md_content = f.read()\n",
    "            html = markdown2.markdown(md_content)\n",
    "            return ''.join(BeautifulSoup(html, 'lxml').stripped_strings)\n",
    "    \n",
    "    def extract_fields(self, text: str, doc_type: str) -> Dict[str, str]:\n",
    "        \"\"\"增强版字段提取\"\"\"\n",
    "        fields = {}\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        for field, pattern in FIELD_PATTERNS.items():\n",
    "            try:\n",
    "                matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    if match.groups():\n",
    "                        # 取第一个非空捕获组\n",
    "                        value = next((g for g in match.groups() if g), \"\")\n",
    "                        if value.strip():\n",
    "                            fields[field] = value.strip()\n",
    "                            break\n",
    "            except Exception as e:\n",
    "                print(f\"字段提取错误 - {field}: {e}\")\n",
    "        \n",
    "        return fields\n",
    "\n",
    "def enhanced_process_all_materials(materials_dir: str, data_jsonl_path: str, model_path: str) -> None:\n",
    "    \"\"\"增强版批量处理函数\"\"\"\n",
    "    detector = EnhancedInsuranceRiskDetector(model_path)\n",
    "    \n",
    "    with open(data_jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        tasks = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"已加载 {len(tasks)} 条检测规则\")\n",
    "    \n",
    "    material_ids = []\n",
    "    for item in os.listdir(materials_dir):\n",
    "        item_path = os.path.join(materials_dir, item)\n",
    "        if os.path.isdir(item_path) and item.startswith(\"m_\"):\n",
    "            material_ids.append(item)\n",
    "    \n",
    "    print(f\"发现 {len(material_ids)} 个素材包\")\n",
    "    \n",
    "    results = []\n",
    "    for task in tasks:\n",
    "        material_id = task[\"material_id\"]\n",
    "        matched_materials = [m for m in material_ids if m.startswith(material_id.rstrip('as'))]\n",
    "        \n",
    "        if not matched_materials:\n",
    "            print(f\"警告：未找到匹配的素材包: {material_id}\")\n",
    "            task[\"result\"] = False\n",
    "            task[\"reason\"] = \"未找到匹配的素材包\"\n",
    "            results.append(task)\n",
    "            continue\n",
    "        \n",
    "        material_path = os.path.join(materials_dir, matched_materials[0])\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n处理任务: {task.get('rule_id', '无规则ID')} - {task['rule']}\")\n",
    "            print(f\"素材包: {material_id}，实际路径: {material_path}\")\n",
    "            \n",
    "            material_data = detector.parse_material(material_path)\n",
    "            result = detector.detect_risk(material_data, task[\"rule\"])\n",
    "            \n",
    "            task[\"result\"] = bool(result)\n",
    "            task[\"reason\"] = \"素材内容一致\" if result else \"素材内容存在冲突\"\n",
    "            results.append(task)\n",
    "            \n",
    "            print(f\"检测结果: {'合规' if result else '违规'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理素材包失败: {material_id}, 错误: {e}\")\n",
    "            task[\"result\"] = False\n",
    "            task[\"reason\"] = f\"处理失败: {str(e)}\"\n",
    "            results.append(task)\n",
    "    \n",
    "    output_path = os.path.join(os.path.dirname(data_jsonl_path), \"enhanced_检测结果.jsonl\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for res in results:\n",
    "            output_data = {\n",
    "                \"material_id\": res[\"material_id\"],\n",
    "                \"rule_id\": res.get(\"rule_id\"),\n",
    "                \"rule\": res[\"rule\"],\n",
    "                \"result\": res[\"result\"],\n",
    "                \"reason\": res.get(\"reason\", \"\")\n",
    "            }\n",
    "            json_line = json.dumps(output_data, ensure_ascii=False)\n",
    "            f_out.write(json_line + \"\\n\")\n",
    "    \n",
    "    print(f\"\\n检测完成，结果已保存至: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    materials_dir = r\"C:\\Users\\27782\\Desktop\\测试 A 集\\materials\"\n",
    "    data_jsonl = r\"C:\\Users\\27782\\Desktop\\测试 A 集\\data.jsonl\"\n",
    "    model_path = r\"C:\\Users\\27782\\Desktop\\BERT\\models--google-bert--bert-base-chinese\\snapshots\\c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\"\n",
    "    \n",
    "    print(\"开始加载语义模型...\")\n",
    "    try:\n",
    "        enhanced_process_all_materials(materials_dir, data_jsonl, model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"程序执行失败: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
